% S. Mohsen Amiri (amiri1982@gmail.com) October 2012
% An example for testing an using softmax cost function and linear
% with softmax cost
% The code is based on Andrew Ng's online course
clc
clear
close all

smOpt.numClasses = 10;     % Number of classes (MNIST images fall into 10 classes)
%%======================================================================
%% STEP 1: Load data
load('dataset/MNIST_dataset.mat')
DEBUG = true; % Set DEBUG to true when debugging.
if DEBUG
    M = 8;
    N = 100;
    inputData = randn(M, N);
    labels = randi(smOpt.numClasses, N, 1);
end
[theta smOpt]= softmaxInit(inputData, smOpt);

smCost = @(x)softmaxCost(x, data, labels, smOpt);
[cost, grad] = smCost(softmaxCost(inputData, labels, smOpt);
%%======================================================================
%% STEP 3: Gradient checking
if DEBUG    
    numGrad = computeNumericalGradient( @(x) softmaxCost(x, numClasses, ...
                                    inputSize, lambda, inputData, labels), theta);

    % Use this to visually compare the gradients side by side
    disp([numGrad grad]); 

    % Compare numerically computed gradients with those computed analytically
    diff = norm(numGrad-grad)/norm(numGrad+grad);
    disp(diff); 
    % The difference should be small. 
    % In our implementation, these values are usually less than 1e-7.

    % When your gradients are correct, congratulations!
end

%%======================================================================
%% STEP 4: Learning parameters
%
%  Once you have verified that your gradients are correct, 
%  you can start training your softmax regression code using softmaxTrain
%  (which uses minFunc).

%[inputData C M] = ZCAwhite(inputData,0.1);

options.maxIter = 100;
softmaxModel = softmaxTrain(inputSize, numClasses, lambda, ...
                            inputData, labels, options);
                          
% Although we only use 100 iterations here to train a classifier for the 
% MNIST data set, in practice, training for more iterations is usually
% beneficial.

%%======================================================================
%% STEP 5: Testing
%
%  You should now test your model against the test images.
%  To do this, you will first need to write softmaxPredict
%  (in softmaxPredict.m), which should return predictions
%  given a softmax model and the input data.

images = loadMNISTImages('./../MNIST/t10k-images.idx3-ubyte');
labels = loadMNISTLabels('./../MNIST/t10k-labels.idx1-ubyte');
labels(labels==0) = 10; % Remap 0 to 10

inputData = images;

% You will have to implement softmaxPredict in softmaxPredict.m
%[inputData] = ZCAwhite(inputData,0.1,C,M);
[pred] = softmaxPredict(softmaxModel, inputData);

acc = mean(labels(:) == pred(:));
fprintf('Accuracy: %0.3f%%\n', acc * 100);

% Accuracy is the proportion of correctly classified images
% After 100 iterations, the results for our implementation were:
%
% Accuracy: 92.200%
%
% If your values are too low (accuracy less than 0.91), you should check 
% your code for errors, and make sure you are training on the 
% entire data set of 60000 28x28 training images 
% (unless you modified the loading code, this should be the case)
